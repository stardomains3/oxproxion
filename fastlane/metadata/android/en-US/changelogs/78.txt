- Add llama.cpp as a locally-served LAN AI served option.